@InProceedings{pmlr-v32-hutter14,
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very high-dimensional cases—most performance variation is attributable to just a few hyperparameters.},
  address = {Bejing, China},
  author = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  editor = {Xing, Eric P. and Jebara, Tony},
  month = {22--24 Jun},
  number = {1},
  pages = {754--762},
  pdf = {http://proceedings.mlr.press/v32/hutter14.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Efficient Approach for Assessing Hyperparameter Importance},
  url = {https://proceedings.mlr.press/v32/hutter14.html},
  volume = {32},
  year = {2014},
}
@InProceedings{pmlr-v188-makarova22a,
  title = 	 {Automatic Termination for Hyperparameter Optimization},
  author =       {Makarova, Anastasia and Shen, Huibin and Perrone, Valerio and Klein, Aaron and Faddoul, Jean Baptiste and Krause, Andreas and Seeger, Matthias and Archambeau, Cedric},
  booktitle = 	 {Proceedings of the First International Conference on Automated Machine Learning},
  pages = 	 {7/1--21},
  year = 	 {2022},
  editor = 	 {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
  volume = 	 {188},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v188/makarova22a/makarova22a.pdf},
  url = 	 {https://proceedings.mlr.press/v188/makarova22a.html},
  abstract = 	 {Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) in machine learning.  At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops  the procedure if it is sufficiently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error. Across an extensive range of real-world HPO problems and baselines, we show that our termination criterion achieves a better trade-off between the test performance and optimization time. Additionally, we find that overfitting may occur in the context of HPO, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets.}
}
